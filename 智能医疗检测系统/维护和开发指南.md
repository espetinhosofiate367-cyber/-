# æ™ºèƒ½åŒ»ç–—æ£€æµ‹ç³»ç»Ÿ - ç»´æŠ¤å’Œå¼€å‘æŒ‡å—

## ğŸ“‹ æŒ‡å—æ¦‚è¿°

æœ¬æŒ‡å—ä¸ºæ™ºèƒ½åŒ»ç–—æ£€æµ‹ç³»ç»Ÿçš„ç»´æŠ¤äººå‘˜å’Œå¼€å‘è€…æä¾›å…¨é¢çš„æŠ€æœ¯æŒ‡å¯¼ï¼ŒåŒ…æ‹¬ç³»ç»Ÿç»´æŠ¤ã€æ•…éšœæ’é™¤ã€åŠŸèƒ½æ‰©å±•ã€æ€§èƒ½ä¼˜åŒ–å’Œå¼€å‘è§„èŒƒç­‰å†…å®¹ã€‚

**ç›®æ ‡è¯»è€…**: ç³»ç»Ÿç®¡ç†å‘˜ã€ç»´æŠ¤å·¥ç¨‹å¸ˆã€è½¯ä»¶å¼€å‘è€…  
**æŠ€èƒ½è¦æ±‚**: Pythonç¼–ç¨‹ã€æœºå™¨å­¦ä¹ åŸºç¡€ã€ç³»ç»Ÿè¿ç»´ç»éªŒ  
**æ–‡æ¡£ç‰ˆæœ¬**: v2.0  
**æœ€åæ›´æ–°**: 2024-01-01

---

## ğŸ”§ ç³»ç»Ÿç»´æŠ¤æŒ‡å—

### 1. æ—¥å¸¸ç»´æŠ¤ä»»åŠ¡

#### 1.1 ç³»ç»Ÿå¥åº·æ£€æŸ¥

**æ¯æ—¥æ£€æŸ¥æ¸…å•**:
```bash
# 1. æ£€æŸ¥ç³»ç»Ÿè¿è¡ŒçŠ¶æ€
python -c "import psutil; print(f'CPU: {psutil.cpu_percent()}%, Memory: {psutil.virtual_memory().percent}%')"

# 2. æ£€æŸ¥ç£ç›˜ç©ºé—´
python -c "import shutil; print(f'Disk usage: {shutil.disk_usage(\".\").used / shutil.disk_usage(\".\").total * 100:.1f}%')"

# 3. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶å¤§å°
dir /s logs\*.log

# 4. éªŒè¯é…ç½®æ–‡ä»¶å®Œæ•´æ€§
python -c "import json; json.load(open('detection_config.json'))"
```

**æ¯å‘¨æ£€æŸ¥æ¸…å•**:
- [ ] æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç¼“å­˜
- [ ] å¤‡ä»½é‡è¦é…ç½®æ–‡ä»¶
- [ ] æ£€æŸ¥ä¾èµ–åº“æ›´æ–°
- [ ] è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] æ£€æŸ¥ç¡¬ä»¶è®¾å¤‡çŠ¶æ€

**æ¯æœˆæ£€æŸ¥æ¸…å•**:
- [ ] ç³»ç»Ÿæ€§èƒ½åˆ†ææŠ¥å‘Š
- [ ] æ•°æ®åº“æ¸…ç†å’Œä¼˜åŒ–
- [ ] å®‰å…¨è¡¥ä¸æ›´æ–°
- [ ] ç”¨æˆ·åé¦ˆæ”¶é›†å’Œåˆ†æ
- [ ] ç³»ç»Ÿå®¹é‡è§„åˆ’è¯„ä¼°

#### 1.2 æ—¥å¿—ç®¡ç†

**æ—¥å¿—æ–‡ä»¶ä½ç½®**:
```
logs/
â”œâ”€â”€ system.log          # ç³»ç»Ÿè¿è¡Œæ—¥å¿—
â”œâ”€â”€ detection.log       # æ£€æµ‹ç®—æ³•æ—¥å¿—
â”œâ”€â”€ communication.log   # ä¸²å£é€šä¿¡æ—¥å¿—
â”œâ”€â”€ error.log          # é”™è¯¯æ—¥å¿—
â””â”€â”€ performance.log    # æ€§èƒ½ç›‘æ§æ—¥å¿—
```

**æ—¥å¿—è½®è½¬é…ç½®**:
```python
import logging
from logging.handlers import RotatingFileHandler

# é…ç½®æ—¥å¿—è½®è½¬
handler = RotatingFileHandler(
    'logs/system.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)

# æ—¥å¿—æ ¼å¼
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
handler.setFormatter(formatter)
```

**æ—¥å¿—åˆ†æè„šæœ¬**:
```python
def analyze_logs():
    """åˆ†æç³»ç»Ÿæ—¥å¿—ï¼Œç”ŸæˆæŠ¥å‘Š"""
    import re
    from collections import Counter
    
    error_patterns = []
    warning_patterns = []
    
    with open('logs/system.log', 'r') as f:
        for line in f:
            if 'ERROR' in line:
                error_patterns.append(line.strip())
            elif 'WARNING' in line:
                warning_patterns.append(line.strip())
    
    print(f"é”™è¯¯æ•°é‡: {len(error_patterns)}")
    print(f"è­¦å‘Šæ•°é‡: {len(warning_patterns)}")
    
    # ç”Ÿæˆé”™è¯¯ç»Ÿè®¡
    error_counter = Counter(error_patterns)
    for error, count in error_counter.most_common(5):
        print(f"  {error}: {count}æ¬¡")
```

#### 1.3 æ•°æ®å¤‡ä»½ç­–ç•¥

**å¤‡ä»½è®¡åˆ’**:
```bash
# æ¯æ—¥å¢é‡å¤‡ä»½è„šæœ¬
@echo off
set BACKUP_DIR=backup\%date:~0,4%%date:~5,2%%date:~8,2%
mkdir %BACKUP_DIR%

# å¤‡ä»½é…ç½®æ–‡ä»¶
copy *.json %BACKUP_DIR%\
copy *.yml %BACKUP_DIR%\

# å¤‡ä»½é‡è¦æ•°æ®
copy *.pkl %BACKUP_DIR%\
copy *.pt %BACKUP_DIR%\

# å‹ç¼©å¤‡ä»½
powershell Compress-Archive -Path %BACKUP_DIR% -DestinationPath %BACKUP_DIR%.zip
```

**å¤‡ä»½éªŒè¯**:
```python
def verify_backup(backup_path):
    """éªŒè¯å¤‡ä»½æ–‡ä»¶å®Œæ•´æ€§"""
    import os
    import json
    
    required_files = [
        'detection_config.json',
        'pulmonary_nodule_training_system.pkl',
        'æ¨¡å‹æƒé‡.pt'
    ]
    
    for file in required_files:
        file_path = os.path.join(backup_path, file)
        if not os.path.exists(file_path):
            print(f"âŒ ç¼ºå°‘å¤‡ä»½æ–‡ä»¶: {file}")
            return False
        
        # éªŒè¯JSONæ–‡ä»¶æ ¼å¼
        if file.endswith('.json'):
            try:
                with open(file_path, 'r') as f:
                    json.load(f)
            except json.JSONDecodeError:
                print(f"âŒ JSONæ–‡ä»¶æ ¼å¼é”™è¯¯: {file}")
                return False
    
    print("âœ… å¤‡ä»½éªŒè¯é€šè¿‡")
    return True
```

### 2. æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–

#### 2.1 æ€§èƒ½ç›‘æ§æŒ‡æ ‡

**å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)**:
```python
class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'detection_latency': [],      # æ£€æµ‹å»¶è¿Ÿ (ms)
            'processing_fps': [],         # å¤„ç†å¸§ç‡ (FPS)
            'memory_usage': [],           # å†…å­˜ä½¿ç”¨ (MB)
            'cpu_usage': [],              # CPUä½¿ç”¨ç‡ (%)
            'detection_accuracy': [],     # æ£€æµ‹å‡†ç¡®ç‡ (%)
            'false_positive_rate': []     # å‡é˜³æ€§ç‡ (%)
        }
    
    def collect_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        import psutil
        import time
        
        # CPUå’Œå†…å­˜ä½¿ç”¨ç‡
        self.metrics['cpu_usage'].append(psutil.cpu_percent())
        self.metrics['memory_usage'].append(
            psutil.virtual_memory().used / 1024 / 1024
        )
        
        # æ£€æµ‹å»¶è¿Ÿæµ‹è¯•
        start_time = time.time()
        # æ‰§è¡Œæ£€æµ‹ç®—æ³•
        self.run_detection_test()
        latency = (time.time() - start_time) * 1000
        self.metrics['detection_latency'].append(latency)
    
    def generate_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        import numpy as np
        
        report = {
            'avg_latency': np.mean(self.metrics['detection_latency']),
            'max_latency': np.max(self.metrics['detection_latency']),
            'avg_memory': np.mean(self.metrics['memory_usage']),
            'max_memory': np.max(self.metrics['memory_usage']),
            'avg_cpu': np.mean(self.metrics['cpu_usage'])
        }
        
        return report
```

**æ€§èƒ½é˜ˆå€¼å‘Šè­¦**:
```python
PERFORMANCE_THRESHOLDS = {
    'max_latency': 200,        # æœ€å¤§å»¶è¿Ÿ 200ms
    'max_memory': 1000,        # æœ€å¤§å†…å­˜ 1GB
    'max_cpu': 80,             # æœ€å¤§CPUä½¿ç”¨ç‡ 80%
    'min_accuracy': 0.90,      # æœ€å°å‡†ç¡®ç‡ 90%
    'max_false_positive': 0.10 # æœ€å¤§å‡é˜³æ€§ç‡ 10%
}

def check_performance_alerts(metrics):
    """æ£€æŸ¥æ€§èƒ½å‘Šè­¦"""
    alerts = []
    
    if metrics['avg_latency'] > PERFORMANCE_THRESHOLDS['max_latency']:
        alerts.append(f"æ£€æµ‹å»¶è¿Ÿè¿‡é«˜: {metrics['avg_latency']:.1f}ms")
    
    if metrics['max_memory'] > PERFORMANCE_THRESHOLDS['max_memory']:
        alerts.append(f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {metrics['max_memory']:.1f}MB")
    
    if metrics['avg_cpu'] > PERFORMANCE_THRESHOLDS['max_cpu']:
        alerts.append(f"CPUä½¿ç”¨ç‡è¿‡é«˜: {metrics['avg_cpu']:.1f}%")
    
    return alerts
```

#### 2.2 æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

**ç®—æ³•ä¼˜åŒ–**:
```python
# 1. ä½¿ç”¨Numba JITç¼–è¯‘åŠ é€Ÿ
from numba import jit

@jit(nopython=True)
def optimized_detection_algorithm(data):
    """JITç¼–è¯‘çš„æ£€æµ‹ç®—æ³•"""
    # æ ¸å¿ƒè®¡ç®—é€»è¾‘
    pass

# 2. å‘é‡åŒ–æ“ä½œ
import numpy as np

def vectorized_processing(sensor_data):
    """å‘é‡åŒ–æ•°æ®å¤„ç†"""
    # ä½¿ç”¨NumPyå‘é‡åŒ–æ“ä½œæ›¿ä»£å¾ªç¯
    return np.vectorize(process_single_point)(sensor_data)

# 3. å†…å­˜æ± ç®¡ç†
class MemoryPool:
    def __init__(self, size=1000):
        self.pool = [np.zeros((12, 8)) for _ in range(size)]
        self.available = list(range(size))
    
    def get_buffer(self):
        if self.available:
            return self.pool[self.available.pop()]
        return np.zeros((12, 8))
    
    def return_buffer(self, buffer):
        buffer.fill(0)  # æ¸…é›¶
        self.available.append(self.pool.index(buffer))
```

**å¤šçº¿ç¨‹ä¼˜åŒ–**:
```python
import threading
import queue
from concurrent.futures import ThreadPoolExecutor

class OptimizedDetectionSystem:
    def __init__(self):
        self.data_queue = queue.Queue(maxsize=100)
        self.result_queue = queue.Queue()
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
    
    def producer_thread(self):
        """æ•°æ®ç”Ÿäº§è€…çº¿ç¨‹"""
        while self.running:
            data = self.read_sensor_data()
            try:
                self.data_queue.put(data, timeout=0.1)
            except queue.Full:
                # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæ—§æ•°æ®
                self.data_queue.get()
                self.data_queue.put(data)
    
    def consumer_thread(self):
        """æ•°æ®æ¶ˆè´¹è€…çº¿ç¨‹"""
        while self.running:
            try:
                data = self.data_queue.get(timeout=1.0)
                # æäº¤åˆ°çº¿ç¨‹æ± å¤„ç†
                future = self.thread_pool.submit(self.process_data, data)
                self.result_queue.put(future)
            except queue.Empty:
                continue
```

### 3. æ•…éšœæ’é™¤æŒ‡å—

#### 3.1 å¸¸è§é—®é¢˜è¯Šæ–­

**é—®é¢˜åˆ†ç±»å’Œè§£å†³æ–¹æ¡ˆ**:

| é—®é¢˜ç±»å‹ | ç—‡çŠ¶ | å¯èƒ½åŸå›  | è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|----------|
| å¯åŠ¨å¤±è´¥ | ç¨‹åºæ— æ³•å¯åŠ¨ | ä¾èµ–ç¼ºå¤±ã€é…ç½®é”™è¯¯ | æ£€æŸ¥ä¾èµ–ã€éªŒè¯é…ç½® |
| é€šä¿¡å¼‚å¸¸ | æ— æ³•æ¥æ”¶æ•°æ® | ä¸²å£é—®é¢˜ã€ç¡¬ä»¶æ•…éšœ | æ£€æŸ¥è¿æ¥ã€é‡å¯è®¾å¤‡ |
| æ£€æµ‹å¼‚å¸¸ | æ£€æµ‹ç»“æœå¼‚å¸¸ | ç®—æ³•å‚æ•°ã€æ¨¡å‹é—®é¢˜ | è°ƒæ•´å‚æ•°ã€é‡æ–°è®­ç»ƒ |
| æ€§èƒ½ä¸‹é™ | å“åº”ç¼“æ…¢ | èµ„æºä¸è¶³ã€ç®—æ³•æ•ˆç‡ | ä¼˜åŒ–ç®—æ³•ã€å¢åŠ èµ„æº |
| ç•Œé¢å¡é¡¿ | GUIæ— å“åº” | çº¿ç¨‹é˜»å¡ã€å†…å­˜æ³„æ¼ | æ£€æŸ¥çº¿ç¨‹ã€æ¸…ç†å†…å­˜ |

**è¯Šæ–­è„šæœ¬**:
```python
def system_diagnosis():
    """ç³»ç»Ÿè¯Šæ–­è„šæœ¬"""
    import os
    import sys
    import importlib
    
    print("=== ç³»ç»Ÿè¯Šæ–­æŠ¥å‘Š ===")
    
    # 1. Pythonç¯å¢ƒæ£€æŸ¥
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"Pythonè·¯å¾„: {sys.executable}")
    
    # 2. ä¾èµ–åº“æ£€æŸ¥
    required_modules = [
        'numpy', 'pandas', 'matplotlib', 'sklearn',
        'scipy', 'tkinter', 'serial', 'cv2'
    ]
    
    missing_modules = []
    for module in required_modules:
        try:
            importlib.import_module(module)
            print(f"âœ… {module}")
        except ImportError:
            print(f"âŒ {module}")
            missing_modules.append(module)
    
    # 3. æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
    required_files = [
        'modern_detection_gui_optimized.py',
        'detection_config.json',
        'pulmonary_nodule_training_system.pkl'
    ]
    
    for file in required_files:
        if os.path.exists(file):
            print(f"âœ… {file}")
        else:
            print(f"âŒ {file}")
    
    # 4. ç¡¬ä»¶æ£€æŸ¥
    try:
        import serial.tools.list_ports
        ports = list(serial.tools.list_ports.comports())
        print(f"å¯ç”¨ä¸²å£: {[port.device for port in ports]}")
    except Exception as e:
        print(f"ä¸²å£æ£€æŸ¥å¤±è´¥: {e}")
    
    # 5. ç”Ÿæˆè¯Šæ–­å»ºè®®
    if missing_modules:
        print(f"\nå»ºè®®: å®‰è£…ç¼ºå¤±æ¨¡å— - pip install {' '.join(missing_modules)}")
```

#### 3.2 é”™è¯¯ä»£ç å’Œè§£å†³æ–¹æ¡ˆ

**é”™è¯¯ä»£ç è¡¨**:
```python
ERROR_CODES = {
    'E001': {
        'description': 'é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯',
        'solution': 'æ£€æŸ¥JSONæ ¼å¼ï¼Œä½¿ç”¨å¤‡ä»½æ–‡ä»¶æ¢å¤'
    },
    'E002': {
        'description': 'ä¸²å£é€šä¿¡å¤±è´¥',
        'solution': 'æ£€æŸ¥è®¾å¤‡è¿æ¥ï¼Œé‡å¯ä¸²å£è®¾å¤‡'
    },
    'E003': {
        'description': 'æ¨¡å‹åŠ è½½å¤±è´¥',
        'solution': 'æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§ï¼Œé‡æ–°ä¸‹è½½æ¨¡å‹'
    },
    'E004': {
        'description': 'å†…å­˜ä¸è¶³',
        'solution': 'å…³é—­å…¶ä»–ç¨‹åºï¼Œå¢åŠ ç³»ç»Ÿå†…å­˜'
    },
    'E005': {
        'description': 'æ£€æµ‹ç®—æ³•å¼‚å¸¸',
        'solution': 'é‡ç½®ç®—æ³•å‚æ•°ï¼Œæ£€æŸ¥è¾“å…¥æ•°æ®æ ¼å¼'
    }
}

def handle_error(error_code, context=None):
    """é”™è¯¯å¤„ç†å‡½æ•°"""
    if error_code in ERROR_CODES:
        error_info = ERROR_CODES[error_code]
        print(f"é”™è¯¯ {error_code}: {error_info['description']}")
        print(f"è§£å†³æ–¹æ¡ˆ: {error_info['solution']}")
        
        # è®°å½•é”™è¯¯æ—¥å¿—
        import logging
        logging.error(f"{error_code}: {error_info['description']} - Context: {context}")
    else:
        print(f"æœªçŸ¥é”™è¯¯ä»£ç : {error_code}")
```

#### 3.3 ç´§æ€¥æ¢å¤ç¨‹åº

**ç³»ç»Ÿæ¢å¤è„šæœ¬**:
```python
def emergency_recovery():
    """ç´§æ€¥æ¢å¤ç¨‹åº"""
    import shutil
    import os
    from datetime import datetime
    
    print("=== ç´§æ€¥æ¢å¤ç¨‹åº ===")
    
    # 1. åˆ›å»ºæ¢å¤ç‚¹
    recovery_dir = f"recovery_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.makedirs(recovery_dir, exist_ok=True)
    
    # 2. å¤‡ä»½å½“å‰çŠ¶æ€
    current_files = [
        'detection_config.json',
        'optimization_cache.json',
        'logs/system.log'
    ]
    
    for file in current_files:
        if os.path.exists(file):
            shutil.copy2(file, recovery_dir)
    
    # 3. æ¢å¤é»˜è®¤é…ç½®
    default_config = {
        "gmm_components": 3,
        "smoothing_sigma": 2.0,
        "sensitivity_threshold": 0.5,
        "min_nodule_area": 5,
        "play_speed": 1000
    }
    
    with open('detection_config.json', 'w') as f:
        json.dump(default_config, f, indent=2)
    
    # 4. æ¸…ç†ç¼“å­˜
    cache_files = ['optimization_cache.json', '__pycache__']
    for cache in cache_files:
        if os.path.exists(cache):
            if os.path.isdir(cache):
                shutil.rmtree(cache)
            else:
                os.remove(cache)
    
    print(f"âœ… ç³»ç»Ÿå·²æ¢å¤åˆ°é»˜è®¤çŠ¶æ€")
    print(f"âœ… åŸå§‹æ–‡ä»¶å·²å¤‡ä»½åˆ°: {recovery_dir}")
```

---

## ğŸš€ å¼€å‘æŒ‡å—

### 4. å¼€å‘ç¯å¢ƒæ­å»º

#### 4.1 å¼€å‘ç¯å¢ƒè¦æ±‚

**ç¡¬ä»¶è¦æ±‚**:
- CPU: Intel i5 æˆ– AMD Ryzen 5 ä»¥ä¸Š
- å†…å­˜: 8GB RAM (æ¨è 16GB)
- å­˜å‚¨: 10GB å¯ç”¨ç©ºé—´
- GPU: å¯é€‰ï¼Œç”¨äºæ·±åº¦å­¦ä¹ åŠ é€Ÿ

**è½¯ä»¶è¦æ±‚**:
```bash
# åŸºç¡€ç¯å¢ƒ
Python 3.8-3.10
Git 2.30+
Visual Studio Code (æ¨è)

# å¼€å‘å·¥å…·
pip install --upgrade pip
pip install virtualenv
pip install black          # ä»£ç æ ¼å¼åŒ–
pip install flake8         # ä»£ç æ£€æŸ¥
pip install pytest         # å•å…ƒæµ‹è¯•
pip install sphinx         # æ–‡æ¡£ç”Ÿæˆ
```

#### 4.2 å¼€å‘ç¯å¢ƒé…ç½®

**è™šæ‹Ÿç¯å¢ƒè®¾ç½®**:
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv medical_detection_dev
cd medical_detection_dev
Scripts\activate  # Windows
# source bin/activate  # Linux/Mac

# å®‰è£…å¼€å‘ä¾èµ–
pip install -r requirements_dev.txt
```

**IDEé…ç½® (VS Code)**:
```json
// .vscode/settings.json
{
    "python.defaultInterpreterPath": "./medical_detection_dev/Scripts/python.exe",
    "python.formatting.provider": "black",
    "python.linting.enabled": true,
    "python.linting.flake8Enabled": true,
    "python.testing.pytestEnabled": true,
    "files.associations": {
        "*.py": "python"
    }
}
```

**Gité…ç½®**:
```bash
# é…ç½®Gité’©å­
git config core.hooksPath .githooks

# åˆ›å»ºé¢„æäº¤é’©å­
echo "#!/bin/sh" > .githooks/pre-commit
echo "black --check ." >> .githooks/pre-commit
echo "flake8 ." >> .githooks/pre-commit
chmod +x .githooks/pre-commit
```

### 5. ä»£ç è§„èŒƒå’Œæœ€ä½³å®è·µ

#### 5.1 ä»£ç é£æ ¼è§„èŒƒ

**Pythonä»£ç è§„èŒƒ (PEP 8)**:
```python
# 1. å¯¼å…¥é¡ºåº
import os                    # æ ‡å‡†åº“
import sys

import numpy as np           # ç¬¬ä¸‰æ–¹åº“
import pandas as pd

from .detection_system import EnhancedDetectionSystem  # æœ¬åœ°æ¨¡å—

# 2. å‘½åè§„èŒƒ
class DetectionSystem:       # ç±»åï¼šå¤§é©¼å³°
    def __init__(self):
        self.sensor_data = None  # å®ä¾‹å˜é‡ï¼šå°å†™+ä¸‹åˆ’çº¿
    
    def detect_nodules(self):    # æ–¹æ³•åï¼šå°å†™+ä¸‹åˆ’çº¿
        """æ£€æµ‹ç»“èŠ‚çš„ä¸»è¦æ–¹æ³•"""
        pass

# 3. å¸¸é‡å®šä¹‰
MAX_DETECTION_THRESHOLD = 0.9    # å¸¸é‡ï¼šå¤§å†™+ä¸‹åˆ’çº¿
DEFAULT_CONFIG_PATH = "config.json"

# 4. å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²
def calculate_risk_score(features: dict) -> float:
    """
    è®¡ç®—é£é™©è¯„åˆ†
    
    Args:
        features (dict): ç‰¹å¾å­—å…¸ï¼ŒåŒ…å«é¢ç§¯ã€åœ†å½¢åº¦ç­‰
        
    Returns:
        float: é£é™©è¯„åˆ† (0.0-1.0)
        
    Raises:
        ValueError: å½“ç‰¹å¾å­—å…¸ä¸ºç©ºæ—¶
        
    Example:
        >>> features = {'area': 10, 'circularity': 0.8}
        >>> score = calculate_risk_score(features)
        >>> print(f"Risk score: {score:.2f}")
    """
    if not features:
        raise ValueError("ç‰¹å¾å­—å…¸ä¸èƒ½ä¸ºç©º")
    
    # è®¡ç®—é€»è¾‘
    return 0.0
```

**ä»£ç è´¨é‡æ£€æŸ¥**:
```bash
# ä»£ç æ ¼å¼åŒ–
black --line-length 88 *.py

# ä»£ç æ£€æŸ¥
flake8 --max-line-length=88 --ignore=E203,W503 *.py

# ç±»å‹æ£€æŸ¥
mypy --ignore-missing-imports *.py

# å¤æ‚åº¦æ£€æŸ¥
radon cc --min B *.py
```

#### 5.2 æ¨¡å—è®¾è®¡åŸåˆ™

**SOLIDåŸåˆ™åº”ç”¨**:
```python
# 1. å•ä¸€èŒè´£åŸåˆ™ (SRP)
class DataProcessor:
    """åªè´Ÿè´£æ•°æ®å¤„ç†"""
    def preprocess_data(self, raw_data):
        pass

class DetectionAlgorithm:
    """åªè´Ÿè´£æ£€æµ‹ç®—æ³•"""
    def detect(self, processed_data):
        pass

# 2. å¼€é—­åŸåˆ™ (OCP)
from abc import ABC, abstractmethod

class BaseDetector(ABC):
    @abstractmethod
    def detect(self, data):
        pass

class GMMDetector(BaseDetector):
    def detect(self, data):
        # GMMæ£€æµ‹å®ç°
        pass

class LSTMDetector(BaseDetector):
    def detect(self, data):
        # LSTMæ£€æµ‹å®ç°
        pass

# 3. ä¾èµ–å€’ç½®åŸåˆ™ (DIP)
class DetectionSystem:
    def __init__(self, detector: BaseDetector):
        self.detector = detector  # ä¾èµ–æŠ½è±¡è€Œéå…·ä½“å®ç°
    
    def run_detection(self, data):
        return self.detector.detect(data)
```

**è®¾è®¡æ¨¡å¼åº”ç”¨**:
```python
# 1. å•ä¾‹æ¨¡å¼ - é…ç½®ç®¡ç†
class ConfigManager:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.config = {}
            self.initialized = True

# 2. è§‚å¯Ÿè€…æ¨¡å¼ - äº‹ä»¶é€šçŸ¥
class EventManager:
    def __init__(self):
        self.observers = []
    
    def subscribe(self, observer):
        self.observers.append(observer)
    
    def notify(self, event):
        for observer in self.observers:
            observer.update(event)

# 3. ç­–ç•¥æ¨¡å¼ - ç®—æ³•é€‰æ‹©
class DetectionStrategy:
    def __init__(self, strategy):
        self.strategy = strategy
    
    def execute(self, data):
        return self.strategy.detect(data)
```

### 6. æµ‹è¯•å’Œè´¨é‡ä¿è¯

#### 6.1 å•å…ƒæµ‹è¯•

**æµ‹è¯•æ¡†æ¶é…ç½®**:
```python
# tests/conftest.py
import pytest
import numpy as np

@pytest.fixture
def sample_sensor_data():
    """æµ‹è¯•ç”¨ä¼ æ„Ÿå™¨æ•°æ®"""
    return np.random.rand(12, 8)

@pytest.fixture
def detection_system():
    """æµ‹è¯•ç”¨æ£€æµ‹ç³»ç»Ÿ"""
    from enhanced_detection_system import EnhancedNoduleDetectionSystem
    return EnhancedNoduleDetectionSystem()
```

**æµ‹è¯•ç”¨ä¾‹ç¤ºä¾‹**:
```python
# tests/test_detection_system.py
import pytest
import numpy as np
from enhanced_detection_system import EnhancedNoduleDetectionSystem

class TestDetectionSystem:
    def test_initialization(self):
        """æµ‹è¯•ç³»ç»Ÿåˆå§‹åŒ–"""
        system = EnhancedNoduleDetectionSystem()
        assert system is not None
        assert hasattr(system, 'gmm_components')
    
    def test_detect_nodules_with_valid_data(self, detection_system, sample_sensor_data):
        """æµ‹è¯•æ­£å¸¸æ•°æ®æ£€æµ‹"""
        result = detection_system.detect_nodules(sample_sensor_data)
        
        assert 'detections' in result
        assert 'risk_scores' in result
        assert isinstance(result['detections'], list)
    
    def test_detect_nodules_with_invalid_data(self, detection_system):
        """æµ‹è¯•å¼‚å¸¸æ•°æ®å¤„ç†"""
        with pytest.raises(ValueError):
            detection_system.detect_nodules(None)
        
        with pytest.raises(ValueError):
            detection_system.detect_nodules(np.array([]))
    
    @pytest.mark.parametrize("threshold", [0.1, 0.5, 0.9])
    def test_different_thresholds(self, detection_system, sample_sensor_data, threshold):
        """æµ‹è¯•ä¸åŒé˜ˆå€¼è®¾ç½®"""
        detection_system.sensitivity_threshold = threshold
        result = detection_system.detect_nodules(sample_sensor_data)
        
        assert result is not None
        # éªŒè¯é˜ˆå€¼å½±å“æ£€æµ‹ç»“æœ
```

**æ€§èƒ½æµ‹è¯•**:
```python
# tests/test_performance.py
import time
import pytest
from enhanced_detection_system import EnhancedNoduleDetectionSystem

class TestPerformance:
    def test_detection_latency(self, detection_system, sample_sensor_data):
        """æµ‹è¯•æ£€æµ‹å»¶è¿Ÿ"""
        start_time = time.time()
        result = detection_system.detect_nodules(sample_sensor_data)
        end_time = time.time()
        
        latency = (end_time - start_time) * 1000  # ms
        assert latency < 100, f"æ£€æµ‹å»¶è¿Ÿè¿‡é«˜: {latency:.2f}ms"
    
    def test_memory_usage(self, detection_system):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # è¿è¡Œå¤šæ¬¡æ£€æµ‹
        for _ in range(100):
            data = np.random.rand(12, 8)
            detection_system.detect_nodules(data)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        assert memory_increase < 50, f"å†…å­˜æ³„æ¼: {memory_increase:.2f}MB"
```

**æµ‹è¯•è¿è¡Œ**:
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest tests/ -v

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_detection_system.py::TestDetectionSystem::test_initialization -v

# ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=. --cov-report=html tests/

# æ€§èƒ½æµ‹è¯•
pytest tests/test_performance.py -v --benchmark-only
```

#### 6.2 é›†æˆæµ‹è¯•

**ç«¯åˆ°ç«¯æµ‹è¯•**:
```python
# tests/test_integration.py
import pytest
import threading
import time
from unittest.mock import Mock, patch

class TestIntegration:
    def test_full_detection_pipeline(self):
        """æµ‹è¯•å®Œæ•´æ£€æµ‹æµç¨‹"""
        # 1. æ¨¡æ‹Ÿä¸²å£æ•°æ®
        mock_serial_data = b'\xA5\x5A' + b'\x00' * 96  # æ¨¡æ‹Ÿåè®®æ•°æ®
        
        # 2. å¯åŠ¨æ£€æµ‹ç³»ç»Ÿ
        from modern_detection_gui_optimized import ModernDetectionGUI
        gui = ModernDetectionGUI()
        
        # 3. æ¨¡æ‹Ÿæ•°æ®è¾“å…¥
        with patch('serial.Serial') as mock_serial:
            mock_serial.return_value.read.return_value = mock_serial_data
            
            # å¯åŠ¨æ£€æµ‹
            gui.start_detection()
            time.sleep(2)  # ç­‰å¾…å¤„ç†
            
            # éªŒè¯ç»“æœ
            assert gui.detection_results is not None
            assert len(gui.detection_results) > 0
    
    def test_concurrent_detection(self):
        """æµ‹è¯•å¹¶å‘æ£€æµ‹"""
        from enhanced_detection_system import EnhancedNoduleDetectionSystem
        
        system = EnhancedNoduleDetectionSystem()
        results = []
        
        def detection_worker():
            data = np.random.rand(12, 8)
            result = system.detect_nodules(data)
            results.append(result)
        
        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹å¹¶å‘æ£€æµ‹
        threads = []
        for _ in range(10):
            thread = threading.Thread(target=detection_worker)
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # éªŒè¯ç»“æœ
        assert len(results) == 10
        for result in results:
            assert result is not None
```

### 7. åŠŸèƒ½æ‰©å±•æŒ‡å—

#### 7.1 æ·»åŠ æ–°çš„æ£€æµ‹ç®—æ³•

**ç®—æ³•æ¥å£å®šä¹‰**:
```python
# algorithms/base_detector.py
from abc import ABC, abstractmethod
from typing import Dict, List, Any

class BaseDetector(ABC):
    """æ£€æµ‹ç®—æ³•åŸºç±»"""
    
    @abstractmethod
    def initialize(self, config: Dict[str, Any]) -> None:
        """åˆå§‹åŒ–ç®—æ³•å‚æ•°"""
        pass
    
    @abstractmethod
    def detect(self, sensor_data: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡Œæ£€æµ‹ç®—æ³•"""
        pass
    
    @abstractmethod
    def get_algorithm_info(self) -> Dict[str, str]:
        """è·å–ç®—æ³•ä¿¡æ¯"""
        pass
```

**æ–°ç®—æ³•å®ç°ç¤ºä¾‹**:
```python
# algorithms/svm_detector.py
from sklearn.svm import OneClassSVM
import numpy as np
from .base_detector import BaseDetector

class SVMDetector(BaseDetector):
    """åŸºäºSVMçš„å¼‚å¸¸æ£€æµ‹ç®—æ³•"""
    
    def __init__(self):
        self.model = None
        self.is_trained = False
    
    def initialize(self, config: Dict[str, Any]) -> None:
        """åˆå§‹åŒ–SVMå‚æ•°"""
        self.model = OneClassSVM(
            kernel=config.get('kernel', 'rbf'),
            gamma=config.get('gamma', 'scale'),
            nu=config.get('nu', 0.1)
        )
    
    def detect(self, sensor_data: np.ndarray) -> Dict[str, Any]:
        """æ‰§è¡ŒSVMæ£€æµ‹"""
        if not self.is_trained:
            # ä½¿ç”¨å†å²æ­£å¸¸æ•°æ®è®­ç»ƒ
            self._train_model(sensor_data)
        
        # é¢„æµ‹å¼‚å¸¸
        flattened_data = sensor_data.flatten().reshape(1, -1)
        prediction = self.model.predict(flattened_data)
        decision_score = self.model.decision_function(flattened_data)
        
        return {
            'is_anomaly': prediction[0] == -1,
            'anomaly_score': abs(decision_score[0]),
            'algorithm': 'SVM',
            'confidence': min(abs(decision_score[0]), 1.0)
        }
    
    def get_algorithm_info(self) -> Dict[str, str]:
        """è·å–ç®—æ³•ä¿¡æ¯"""
        return {
            'name': 'SVM Detector',
            'version': '1.0.0',
            'description': 'åŸºäºæ”¯æŒå‘é‡æœºçš„å¼‚å¸¸æ£€æµ‹ç®—æ³•',
            'author': 'Development Team'
        }
    
    def _train_model(self, data: np.ndarray) -> None:
        """è®­ç»ƒSVMæ¨¡å‹"""
        # è¿™é‡Œåº”è¯¥ä½¿ç”¨å†å²æ­£å¸¸æ•°æ®è¿›è¡Œè®­ç»ƒ
        # ä¸ºæ¼”ç¤ºç›®çš„ï¼Œä½¿ç”¨å½“å‰æ•°æ®
        flattened_data = data.flatten().reshape(1, -1)
        self.model.fit(flattened_data)
        self.is_trained = True
```

**ç®—æ³•æ³¨å†Œå’Œç®¡ç†**:
```python
# algorithms/algorithm_manager.py
from typing import Dict, Type
from .base_detector import BaseDetector
from .gmm_detector import GMMDetector
from .lstm_detector import LSTMDetector
from .svm_detector import SVMDetector

class AlgorithmManager:
    """ç®—æ³•ç®¡ç†å™¨"""
    
    def __init__(self):
        self.algorithms: Dict[str, Type[BaseDetector]] = {
            'gmm': GMMDetector,
            'lstm': LSTMDetector,
            'svm': SVMDetector
        }
    
    def register_algorithm(self, name: str, algorithm_class: Type[BaseDetector]):
        """æ³¨å†Œæ–°ç®—æ³•"""
        self.algorithms[name] = algorithm_class
    
    def create_detector(self, algorithm_name: str, config: Dict) -> BaseDetector:
        """åˆ›å»ºæ£€æµ‹å™¨å®ä¾‹"""
        if algorithm_name not in self.algorithms:
            raise ValueError(f"æœªçŸ¥ç®—æ³•: {algorithm_name}")
        
        detector = self.algorithms[algorithm_name]()
        detector.initialize(config)
        return detector
    
    def list_algorithms(self) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨ç®—æ³•"""
        return list(self.algorithms.keys())
```

#### 7.2 æ·»åŠ æ–°çš„æ•°æ®æº

**æ•°æ®æºæ¥å£**:
```python
# data_sources/base_source.py
from abc import ABC, abstractmethod
from typing import Generator, Dict, Any
import numpy as np

class BaseDataSource(ABC):
    """æ•°æ®æºåŸºç±»"""
    
    @abstractmethod
    def connect(self, config: Dict[str, Any]) -> bool:
        """è¿æ¥æ•°æ®æº"""
        pass
    
    @abstractmethod
    def disconnect(self) -> None:
        """æ–­å¼€è¿æ¥"""
        pass
    
    @abstractmethod
    def read_data(self) -> np.ndarray:
        """è¯»å–æ•°æ®"""
        pass
    
    @abstractmethod
    def is_connected(self) -> bool:
        """æ£€æŸ¥è¿æ¥çŠ¶æ€"""
        pass
```

**ç½‘ç»œæ•°æ®æºå®ç°**:
```python
# data_sources/network_source.py
import socket
import struct
import numpy as np
from .base_source import BaseDataSource

class NetworkDataSource(BaseDataSource):
    """ç½‘ç»œæ•°æ®æº"""
    
    def __init__(self):
        self.socket = None
        self.connected = False
    
    def connect(self, config: Dict[str, Any]) -> bool:
        """è¿æ¥ç½‘ç»œæ•°æ®æº"""
        try:
            self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.socket.connect((config['host'], config['port']))
            self.connected = True
            return True
        except Exception as e:
            print(f"ç½‘ç»œè¿æ¥å¤±è´¥: {e}")
            return False
    
    def disconnect(self) -> None:
        """æ–­å¼€ç½‘ç»œè¿æ¥"""
        if self.socket:
            self.socket.close()
            self.connected = False
    
    def read_data(self) -> np.ndarray:
        """ä»ç½‘ç»œè¯»å–æ•°æ®"""
        if not self.connected:
            raise ConnectionError("æœªè¿æ¥åˆ°æ•°æ®æº")
        
        try:
            # è¯»å–æ•°æ®é•¿åº¦
            length_data = self.socket.recv(4)
            data_length = struct.unpack('I', length_data)[0]
            
            # è¯»å–å®é™…æ•°æ®
            data_bytes = self.socket.recv(data_length)
            data_array = np.frombuffer(data_bytes, dtype=np.float32)
            
            # é‡å¡‘ä¸º12x8çŸ©é˜µ
            return data_array.reshape(12, 8)
        except Exception as e:
            print(f"æ•°æ®è¯»å–å¤±è´¥: {e}")
            return np.zeros((12, 8))
    
    def is_connected(self) -> bool:
        """æ£€æŸ¥è¿æ¥çŠ¶æ€"""
        return self.connected
```

#### 7.3 æ·»åŠ æ–°çš„å¯è§†åŒ–ç»„ä»¶

**å¯è§†åŒ–ç»„ä»¶åŸºç±»**:
```python
# visualization/base_visualizer.py
from abc import ABC, abstractmethod
import matplotlib.pyplot as plt
from typing import Any, Dict

class BaseVisualizer(ABC):
    """å¯è§†åŒ–ç»„ä»¶åŸºç±»"""
    
    def __init__(self, figure, axes):
        self.figure = figure
        self.axes = axes
    
    @abstractmethod
    def update(self, data: Any, **kwargs) -> None:
        """æ›´æ–°å¯è§†åŒ–"""
        pass
    
    @abstractmethod
    def clear(self) -> None:
        """æ¸…é™¤å¯è§†åŒ–"""
        pass
    
    @abstractmethod
    def get_config_options(self) -> Dict[str, Any]:
        """è·å–é…ç½®é€‰é¡¹"""
        pass
```

**3Då¯è§†åŒ–ç»„ä»¶**:
```python
# visualization/volume_visualizer.py
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from .base_visualizer import BaseVisualizer

class VolumeVisualizer(BaseVisualizer):
    """3Dä½“ç§¯å¯è§†åŒ–ç»„ä»¶"""
    
    def __init__(self, figure, axes):
        super().__init__(figure, axes)
        self.scatter = None
    
    def update(self, data: np.ndarray, **kwargs) -> None:
        """æ›´æ–°3Då¯è§†åŒ–"""
        self.clear()
        
        # åˆ›å»º3Dåæ ‡
        x, y = np.meshgrid(range(data.shape[1]), range(data.shape[0]))
        z = data
        
        # åˆ›å»º3Dæ•£ç‚¹å›¾
        self.scatter = self.axes.scatter(
            x.flatten(), 
            y.flatten(), 
            z.flatten(),
            c=z.flatten(),
            cmap=kwargs.get('colormap', 'viridis'),
            s=kwargs.get('point_size', 50),
            alpha=kwargs.get('alpha', 0.7)
        )
        
        # è®¾ç½®æ ‡ç­¾
        self.axes.set_xlabel('X Position')
        self.axes.set_ylabel('Y Position')
        self.axes.set_zlabel('Sensor Value')
        self.axes.set_title('3D Sensor Data Visualization')
        
        self.figure.canvas.draw()
    
    def clear(self) -> None:
        """æ¸…é™¤3Då¯è§†åŒ–"""
        if self.scatter:
            self.scatter.remove()
            self.scatter = None
    
    def get_config_options(self) -> Dict[str, Any]:
        """è·å–é…ç½®é€‰é¡¹"""
        return {
            'colormap': ['viridis', 'plasma', 'turbo', 'coolwarm'],
            'point_size': {'min': 10, 'max': 200, 'default': 50},
            'alpha': {'min': 0.1, 'max': 1.0, 'default': 0.7}
        }
```

### 8. éƒ¨ç½²å’Œå‘å¸ƒ

#### 8.1 æ‰“åŒ…å’Œåˆ†å‘

**ä½¿ç”¨PyInstalleræ‰“åŒ…**:
```bash
# å®‰è£…PyInstaller
pip install pyinstaller

# åˆ›å»ºæ‰“åŒ…é…ç½®æ–‡ä»¶
pyi-makespec --onefile --windowed --icon=ç¨‹åºå›¾æ ‡.ico æ™ºèƒ½æ£€æµ‹ä¸»ç¨‹åº.py

# ä¿®æ”¹specæ–‡ä»¶æ·»åŠ æ•°æ®æ–‡ä»¶
# æ™ºèƒ½æ£€æµ‹ä¸»ç¨‹åº.spec
a = Analysis(['æ™ºèƒ½æ£€æµ‹ä¸»ç¨‹åº.py'],
             pathex=['.'],
             binaries=[],
             datas=[
                 ('ç¨‹åºé…ç½®.json', '.'),
                 ('æ¨¡å‹æƒé‡.pt', '.'),
                 ('æ£€æµ‹ç±»åˆ«æ ‡ç­¾.txt', '.'),
                 ('ä¾èµ–æ¸…å•.txt', '.')
             ],
             hiddenimports=[],
             hookspath=[],
             runtime_hooks=[],
             excludes=[],
             win_no_prefer_redirects=False,
             win_private_assemblies=False,
             cipher=block_cipher,
             noarchive=False)

# æ‰§è¡Œæ‰“åŒ…
pyinstaller æ™ºèƒ½æ£€æµ‹ä¸»ç¨‹åº.spec
```

**Dockerå®¹å™¨åŒ–**:
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨æ–‡ä»¶
COPY . .

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app
ENV DISPLAY=:0

# æš´éœ²ç«¯å£
EXPOSE 8080

# å¯åŠ¨å‘½ä»¤
CMD ["python", "æ™ºèƒ½æ£€æµ‹ä¸»ç¨‹åº.py"]
```

**æ„å»ºå’Œè¿è¡ŒDocker**:
```bash
# æ„å»ºé•œåƒ
docker build -t medical-detection-system .

# è¿è¡Œå®¹å™¨
docker run -d \
  --name medical-detection \
  -p 8080:8080 \
  -v /dev/ttyUSB0:/dev/ttyUSB0 \
  --device=/dev/ttyUSB0 \
  medical-detection-system
```

#### 8.2 ç‰ˆæœ¬ç®¡ç†

**è¯­ä¹‰åŒ–ç‰ˆæœ¬æ§åˆ¶**:
```python
# version.py
__version__ = "2.1.0"
__version_info__ = (2, 1, 0)

# ç‰ˆæœ¬å·æ ¼å¼: MAJOR.MINOR.PATCH
# MAJOR: ä¸å…¼å®¹çš„APIä¿®æ”¹
# MINOR: å‘åå…¼å®¹çš„åŠŸèƒ½æ€§æ–°å¢
# PATCH: å‘åå…¼å®¹çš„é—®é¢˜ä¿®æ­£
```

**å‘å¸ƒæ£€æŸ¥æ¸…å•**:
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] ä»£ç è¦†ç›–ç‡ > 80%
- [ ] æ–‡æ¡£æ›´æ–°å®Œæˆ
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡
- [ ] å®‰å…¨æ‰«æé€šè¿‡
- [ ] ä¾èµ–åº“æ›´æ–°åˆ°æœ€æ–°ç¨³å®šç‰ˆ
- [ ] æ‰“åŒ…æµ‹è¯•æˆåŠŸ
- [ ] éƒ¨ç½²æµ‹è¯•æˆåŠŸ

---

## ğŸ“š å‚è€ƒèµ„æº

### æŠ€æœ¯æ–‡æ¡£
- [Pythonå®˜æ–¹æ–‡æ¡£](https://docs.python.org/3/)
- [NumPyç”¨æˆ·æŒ‡å—](https://numpy.org/doc/stable/user/)
- [Scikit-learnæ–‡æ¡£](https://scikit-learn.org/stable/)
- [Matplotlibæ•™ç¨‹](https://matplotlib.org/stable/tutorials/index.html)

### å¼€å‘å·¥å…·
- [Visual Studio Code](https://code.visualstudio.com/)
- [Gitç‰ˆæœ¬æ§åˆ¶](https://git-scm.com/doc)
- [Dockerå®¹å™¨åŒ–](https://docs.docker.com/)
- [pytestæµ‹è¯•æ¡†æ¶](https://docs.pytest.org/)

### æœ€ä½³å®è·µ
- [PEP 8 Pythonä»£ç é£æ ¼æŒ‡å—](https://pep8.org/)
- [Google Pythoné£æ ¼æŒ‡å—](https://google.github.io/styleguide/pyguide.html)
- [Clean CodeåŸåˆ™](https://clean-code-developer.com/)

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### è”ç³»æ–¹å¼
- **æŠ€æœ¯æ”¯æŒé‚®ç®±**: support@medical-detection.com
- **å¼€å‘è€…è®ºå›**: https://forum.medical-detection.com
- **GitHubä»“åº“**: https://github.com/medical-detection/system
- **æ–‡æ¡£ç½‘ç«™**: https://docs.medical-detection.com

### æ”¯æŒæ—¶é—´
- **å·¥ä½œæ—¥**: 9:00-18:00 (GMT+8)
- **ç´§æ€¥æ”¯æŒ**: 24/7 (ä»…é™ç”Ÿäº§ç¯å¢ƒé—®é¢˜)
- **å“åº”æ—¶é—´**: 
  - ç´§æ€¥é—®é¢˜: 2å°æ—¶å†…
  - ä¸€èˆ¬é—®é¢˜: 24å°æ—¶å†…
  - åŠŸèƒ½è¯·æ±‚: 72å°æ—¶å†…

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0  
**æœ€åæ›´æ–°**: 2024-01-01  
**ç»´æŠ¤è€…**: AI Assistant  
**å®¡æ ¸è€…**: æŠ€æœ¯å›¢é˜Ÿ